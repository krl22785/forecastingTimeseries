311 Calls in 2015 
==========
*Kenneth Luna*

Going to use several packages for this project.  "plyr" allows for data summarization while ggplot improves on visualization capabilities. 

```{r}
library("forecast")
library("plyr") ## for data wrangling 
library("ggplot2") ## for data visualizations 
```

The dataset is the number of calls to 311 services in NYC pertaining to housing complaints.  The dataset is from Jan 1 '13 until today.  In aggregate, the dataset has approximately 1.4M records.  

The initial dataset is a record of each call with a date and timestamp.  Preprocessing required grouping this dataset by day, resulting in about 820 records.  

```{r}
data <- read.csv("/Users/krluna/forecastingTimeseries/Project1/311_data.csv")

data$count <- 1
data$Created.Date <- as.Date(data$Created.Date, "%m/%d/%Y")
tempdata <- ddply(data, c("Created.Date"), summarise, 
                 calls = length(count)
                 )

newdata <- tempdata[-c(821),]
#newdata <- tempdata1[-c(1,2,3,4),]

date <- newdata$Created.Date
time <- 1:length(date)
calls <- newdata$calls
```

We now plot our initial data for exploration.  

```{r}

par(mfrow=c(1,1))
plot(date, calls, type="l",
     xlab="Date", ylab="311 Calls")

par(mfrow=c(1,2))
Acf(calls)
Pacf(calls)
```

Preliminary Observations

311 Calls do appear to have seasonality from Jan '13 until today.  What we observe is an increase in calls to 311 during the winter months, likely driven by heating complaints, and then realize a signficant drop off during the spring, summer and early fall months.  

We also observe several shocks in our dataset.  These shocks are call volumes that are significantly higher than that of the average.  These are likely driven by snow storms.  

There does appear to be some fluctation in call volume based on the day of the week.  The plot shows bumps with the occasional drop off each 6 or 7 days.  This trend holds true across the entire, minus the 3 or 4 observations that appear to be days that realized high call volumes. 

The current ACF and PACF models don't tell us much.  As a result, we will be using log(call) values and seasonally adjust our observations.  

Remove Seasonality 

```{r}

log.calls <- log(calls)
diff.log.calls <- c(NA, diff(log.calls))
diff2.log.calls <- c(NA, diff(diff.log.calls))

```


```{r}

par(mfrow=c(1,1))
plot(date, log.calls, type="l",
     xlab="Date", ylab="Log(Calls)")

par(mfrow=c(1,2))
Acf(log.calls)
Pacf(log.calls)

```

We decided to take logs instead of using absolute values in order to scale down some of the spikes in call volumes occurring in January and Febuary.  The two highest days in 311 calls were more than 3x larger than the average call volumne per day for this specific time period.  

Based on our observation of log(calls), we continue to see a high level of seasonality.  Simliar to the prior plot, there are drops in 311 calls during the summer months and it picks up come colder weather.  As a result of this seasonality, our current plot is not stationary and will require seasonal adjustment.  

We will take the monthly averages of the log(call) values and subtract that from each observation.  

```{r}

month <- factor(months(date), levels=format(ISOdate(2000,1:12,1),"%B"))
month.avg <- c(by(log.calls, month, mean))

print(month.avg)

avgSteps <- stepfun(1:11, month.avg, f = 0)
plot(avgSteps, 
     xlab = "Monthly Average", 
     ylab = "Month", 
     main = "Monthly Log(Call) Averages")

```

The following plot is a stepwise representation of the monthly averages across the 2+ years.  We decided to take the monthly average across all the years instead of each month-year pairing because there was trend overtime, therefore activity in a given month and year were similar to that of the same month in a different year.  

```{r}

log.calls.adj <- log.calls - month.avg[month]

par(mfrow=c(1,1))
plot(date, log.calls.adj, type="l",
     xlab="Date", ylab="Log(Calls) Adj.")

par(mfrow=c(1,2))
Acf(log.calls.adj)
Pacf(log.calls.adj)
```



```{r}

diff.log.calls.adj <- c(NA, diff(log.calls.adj))
diff2.log.calls.adj <- c(NA, diff(diff.log.calls.adj))

```


Take the first difference:

```{r}

par(mfrow=c(1,1))
plot(date, diff.log.calls.adj, type="l",
     xlab="Date", ylab="Diff. Log(Calls) Adj.")

par(mfrow=c(1,2))
Acf(diff.log.calls.adj)
Pacf(diff.log.calls.adj)

```

Take teh second difference: 

```{r}

par(mfrow=c(1,1))
plot(date, diff2.log.calls.adj, type="l",
     xlab="Date", ylab="Diff2. Log(Calls)")

par(mfrow=c(1,2))
Acf(diff2.log.calls.adj)
Pacf(diff2.log.calls.adj)

```

Based on the ACF and PACF models for the first difference of log 311 calls, it appears we might have over-differenced as the first lag auto-correlation has a negative value.  The same holds true for the second difference.  

Based on this preliminary analysis, here are the AICC values for the models under consideration:


```{r}

fit.00 <- Arima(log.calls.adj, c(0, 0, 0), include.constant=FALSE)
fit.01 <- Arima(log.calls.adj, c(0, 0, 1), include.constant=FALSE)
fit.02 <- Arima(log.calls.adj, c(0, 0, 2), include.constant=FALSE)
fit.10 <- Arima(log.calls.adj, c(1, 0, 0), include.constant=FALSE)
fit.11 <- Arima(log.calls.adj, c(1, 0, 1), include.constant=FALSE)
fit.12 <- Arima(log.calls.adj, c(1, 0, 2), include.constant=FALSE)
fit.20 <- Arima(log.calls.adj, c(2, 0, 0), include.constant=FALSE)
fit.21 <- Arima(log.calls.adj, c(2, 0, 1), include.constant=FALSE)
fit.22 <- Arima(log.calls.adj, c(2, 0, 2), include.constant=FALSE)

# With constant:
fit.00c <- Arima(log.calls.adj, c(0, 0, 0), include.constant=TRUE)
fit.01c <- Arima(log.calls.adj, c(0, 0, 1), include.constant=TRUE)
fit.02c <- Arima(log.calls.adj, c(0, 0, 2), include.constant=TRUE)
fit.10c <- Arima(log.calls.adj, c(1, 0, 0), include.constant=TRUE)
fit.11c <- Arima(log.calls.adj, c(1, 0, 1), include.constant=TRUE)
fit.12c <- Arima(log.calls.adj, c(1, 0, 2), include.constant=TRUE)
fit.20c <- Arima(log.calls.adj, c(2, 0, 0), include.constant=TRUE)
fit.21c <- Arima(log.calls.adj, c(2, 0, 1), include.constant=TRUE)
fit.22c <- Arima(log.calls.adj, c(2, 0, 2), include.constant=TRUE)

models <- data.frame(p = rep(c(0, 0, 0, 1, 1, 1, 2, 2, 2), 2),
                     d = rep(0, 18),
                     q = rep(c(0, 1, 2), 6),
                     include.constant = c(rep(FALSE, 9), rep(TRUE, 9)),
                     loglik = c(fit.00$loglik, fit.01$loglik, fit.02$loglik,
                                fit.10$loglik, fit.11$loglik, fit.12$loglik,
                                fit.20$loglik, fit.21$loglik, fit.22$loglik,
                                fit.00c$loglik, fit.01c$loglik, fit.02c$loglik,
                                fit.10c$loglik, fit.11c$loglik, fit.12c$loglik,
                                fit.20c$loglik, fit.21c$loglik, fit.22c$loglik),
                     aicc = c(fit.00$aicc, fit.01$aicc, fit.02$aicc,
                                fit.10$aicc, fit.11$aicc, fit.12$aicc,
                                fit.20$aicc, fit.21$aicc, fit.22$aicc,
                                fit.00c$aicc, fit.01c$aicc, fit.02c$aicc,
                                fit.10c$aicc, fit.11c$aicc, fit.12c$aicc,
                                fit.20c$aicc, fit.21c$aicc, fit.22c$aicc)
                     )
```


The p, d, q selected based on AICC is the ARIMA(2, 0, 2) without a constang with an AICC of 735.5825.   

Here is the estimated model:
```{r}

fit <- Arima(log.calls.adj, c(2,0,2), include.constant = FALSE)
print(fit)

(1-pnorm(abs(fit$coef)/sqrt(diag(fit$var.coef))))*2

```

Based on the coefficient values and the standard errors, it appears the ma1 coefficient is not statistically significant.  It has a z-value of less than 2 and its p-value is .2.  

x(t) = .7921 * x(t-1) + -.3662 * x(t-2) + eps(t) + -.2282 * eps(t-1) + -.1220 * eps(t-2)


```{r}

resid <- residuals(fit)

Box.test(resid, lag=12, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=24, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=36, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=48, type = "Ljung-Box", fitdf=4)

```

The small p-values indicate dependence. 

```{r}
par(mfrow=c(1,1))
plot(date, resid, type="l")

par(mfrow=c(1,2))
Acf(resid)
Pacf(resid)
```

Forecast 

```{r}
par(mfrow=c(1,1))
plot(forecast(fit, h=30, level = 95))
```



Part E
------

Here are the Ljung-Box statistics for lack of fit:

```{r}

resid <- residuals(fit)

Box.test(resid, lag=12, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=24, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=36, type = "Ljung-Box", fitdf=4)

Box.test(resid, lag=48, type = "Ljung-Box", fitdf=4)

```

**Does the model seem to be adequate?**

The small p-values indicate dependence, therefore I would conclude that this model is inadequate for this analysis.  

Part F
------

Here is a plot of the residuals, along with the ACF and the PACF of the
residuals:

```{r}
par(mfrow=c(1,1))
plot(date, resid, type="l")

par(mfrow=c(1,2))
Acf(resid)
Pacf(resid)
```

**Do these plots indicate any inadequacies in the model?**

The plot for residuals on the fitted model seems to have a zero mean from just looking at it.  Based on these ACF and PACF plots, there does seem to be some inadequecies as some spikes extend outside of the significance limits, indicating they are more than just white noise.  


Part G
------

Here is the original data, along with the forecasts and 95% forecast intervals
for lead times 1 to 30:

```{r}
par(mfrow=c(1,1))
plot(forecast(fit, h=30, level = 95))
```

**Do the forecasts seem reasonable?**

The forecasts seems reasonable.  It shows a continuation of the trend for the last obserations with a drop in unemployment. 

**Do the forecast intervals seem excessively wide?**

I do believe the forecast seems a bit wide.  The bottom end of the 95% CI goes as low as the lowest record unemployment recording in this data and as high as the highest.  



